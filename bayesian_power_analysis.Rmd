---
title: "Preferring Politeness - Bayesian Power Analysis"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

```{r load libraries}

library(BayesFactor)
library(dplyr)
library(tibble)
library(tidybayes)
library(ggplot2)
library(tidyr)
library(mgcv)
library(stringi)
library(VGAM)
library(base)
library(brms)
library(broom)
library(glmmfields)
library(generics)
library(broom.mixed)
library(tidyverse)
library(tibble)

```

```{r set values}

#The probability of success (prob) is comparable to the effect size in a frequentist power analysis. NOTE: I am not sure how to compute a value for the probability of success given that we are trying to detect a moderate effect.

#Once probability of success is set, we can vary N (number of participants in each age category) until the final output is our desired power.

N <- 20
success_probability <- 0.8

```


```{r simulate data}

#Simulate data.

set.seed(3)

d <-tibble(y=rbinom(n=N, size=1, prob=success_probability))

str(d)

```

```{r get prior}

#This code returns the brms default prior for intercept-only logistic regression models. I don't understand the output, but according to the internet, it's a liberal prior. 

#"y|trials(1)~1" indicates that each y value corresponds to one trial, representing n=1 of the sample.

get_prior(formula=y|trials(1)~1, data=d, family=binomial)

```

```{r model with iterations}

#Model data using a more skeptical prior of normal(0, 2). 

#NOTE: Mathematically, I'm not sure what role this prior has. It will probably have to be adjusted.

fit <- brm(data=d, family=binomial, y|trials(1)~1, prior(normal(0,2), class=Intercept), seed=3)

```

```{r print model}

print(fit)

```

```{r probability metric}

#Because the intercept is returned as log odds, transform it to a probability metric.

fixef(fit)["Intercept", 1] %>% 
  inv_logit_scaled()

```

```{r all draws}

#Extract all the posterior draws and transform them from log odds to probability metrics.

probability_draws <- posterior_samples(fit) %>% 
  transmute(p=inv_logit_scaled(b_Intercept))

probability_draws

```
```{r visualize}

#Visualize the probability metric of the posterior draws.

  ggplot(probability_draws, aes(x=p)) +
  geom_density(fill="grey30") +
  labs(title="Density of probability of selecting a polite speaker", x="Probability of selecting a polite speaker") +
  scale_x_continuous(limits=c(0, 1)) +
  scale_y_continuous(NULL, breaks=NULL) +
  theme_classic()

```

```{r intervals}

#Calculate the posterior median and 95% credible interval.

median_qi(probability_draws)

```

```{r simulation function}

#Write a simulation function for the power analysis.

s_data <- function(seed, n_participants) {
  
  n_trials <- 1
  prob_hit <- success_probability
  
  set.seed(seed)
  
  d <- tibble(y = rbinom(n = n_participants, size = n_trials, prob = prob_hit))
  
  update(fit, newdata = d, seed = seed) %>% 
  posterior_samples() %>% 
  transmute(p = inv_logit_scaled(b_Intercept)) %>% 
  median_qi() %>% 
  select(.lower:.upper)
  
}
```

```{r run simulation function}

#Run the simulation.

#Argument "seed" should be set to 1:1000+. I temporarily set it to 1:10 for ease running and rerunning simulation while editing code. 

sim1 <- 
  tibble(seed=1:10) %>% 
  mutate(ci=map(seed, s_data, n_participants=N)) %>% 
  unnest(cols=c(ci))

```

```{r power}

#Compute power. 

#In the function "mean", ".upper" must be set to equal less than "prob" and "prob_hit" in code chunks {r simulate data} and {r simulation function}, respectively.

#I'm hoping that I've made an error somewhere... Am I correct in saying this output indicates that even if 80% of children in a particular age group possess preference for a polite speaker, with 60 participants we can only expect to detect the effect with 20% power?

summarise(sim1, "power" = mean(.upper < success_probability))

```
